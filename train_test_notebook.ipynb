{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DeepTransGAN: Training and Prediction Notebook\n",
    "This notebook provides a comprehensive guide to training and using a neural network for converting low-light (INR) images to normal-light (RGB) images. The project is inspired by AI enhancement techniques and aims to improve visibility in low-light conditions for applications such as autonomous driving and surveillance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Dataset Preparation\n",
    "\n",
    "The dataset consists of pairs of INR images (low-light) and corresponding RGB images (normal light). The `datasets/data_set.py` script is used to load and preprocess the data.\n",
    "\n",
    "### 1.1. Dataset Format\n",
    "\n",
    "The dataset should be organized as follows:\n",
    "\n",
    "```\n",
    "your_data_directory/\n",
    "    our485/  # Training data\n",
    "        high/  # Contains RGB images\n",
    "            image1.png\n",
    "            image2.jpg\n",
    "            ...\n",
    "        low/   # Contains INR images\n",
    "            image1.png\n",
    "            image2.jpg\n",
    "            ...\n",
    "    eval15/  # Testing data\n",
    "        high/\n",
    "            image1.png\n",
    "            image2.jpg\n",
    "            ...\n",
    "        low/\n",
    "            image1.png\n",
    "            image2.jpg\n",
    "            ...\n",
    "```\n",
    "\n",
    "### 1.2. Loading and Visualizing Data\n",
    "\n",
    "Here's an example of how to use `datasets/data_set.py` to load and visualize the data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import matplotlib.pyplot as plt\n",
    "from torchvision import transforms\n",
    "from datasets.data_set import LowLightDataset\n",
    "\n",
    "# 1. Set the dataset directory\n",
    "data_dir = \"../datasets/LOLdataset\"  # Replace with your dataset path\n",
    "\n",
    "# 2. Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((256, 256)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 3. Create dataset instance\n",
    "train_dataset = LowLightDataset(image_dir=data_dir, transform=transform, phase=\"train\")\n",
    "\n",
    "# 4. Access a sample\n",
    "low_img, high_img = train_dataset[0]\n",
    "\n",
    "# 5. Convert tensors to numpy arrays and rescale\n",
    "low_img_np = low_img.permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "high_img_np = high_img.permute(1, 2, 0).numpy() * 0.5 + 0.5\n",
    "\n",
    "# 6. Display the images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(low_img_np)\n",
    "axes[0].set_title(\"Low-light (INR) Image\")\n",
    "axes[1].imshow(high_img_np)\n",
    "axes[1].set_title(\"Normal-light (RGB) Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Model Definition\n",
    "\n",
    "The core of the INR2RGB conversion is a Generator model, which attempts to transform the low-light INR image into a corresponding RGB image. A Discriminator (or Critic in WGAN) model is used to evaluate the quality of the generated images and provide feedback to the Generator during training.\n",
    "\n",
    "The models are defined in `models/base_mode.py`. The Generator uses RepViT blocks, SPPELAN, and other convolutional layers to extract features and generate the RGB image. The Discriminator (or Critic) uses Disconv layers to classify images as real or fake.\n",
    "\n",
    "### 2.1. Generator\n",
    "\n",
    "The Generator architecture consists of several convolutional blocks, upsampling layers, and concatenation operations. It takes an INR image as input and outputs an RGB image.\n",
    "\n",
    "### 2.2. Discriminator (or Critic)\n",
    "\n",
    "The Discriminator (or Critic) is a binary classifier that distinguishes between real RGB images and generated RGB images. It provides feedback to the Generator, guiding it to produce more realistic images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from models.base_mode import Generator, Discriminator, Critic\n",
    "\n",
    "# Example instantiation\n",
    "generator = Generator()\n",
    "discriminator = Discriminator()\n",
    "# For WGAN training, use Critic instead of Discriminator\n",
    "# critic = Critic()\n",
    "\n",
    "# Print model architecture\n",
    "print(\"Generator architecture:\")\n",
    "print(generator)\n",
    "print(\"\\nDiscriminator architecture:\")\n",
    "print(discriminator)\n",
    "# print(\"\\nCritic architecture:\")\n",
    "# print(critic)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Training\n",
    "\n",
    "The `train.py` script is used to train the INR2RGB model. It defines the training loop, loss functions, optimizers, and other training parameters.\n",
    "\n",
    "### 3.1. Training Process\n",
    "\n",
    "The training process involves the following steps:\n",
    "\n",
    "1.  **Load the dataset**: The `LowLightDataset` class is used to load the INR and RGB image pairs.\n",
    "2.  **Define the models**: The Generator and Discriminator (or Critic) models are instantiated.\n",
    "3.  **Define the loss functions**: The BCEBlurWithLogitsLoss (or MSELoss) is used for the Generator, and BCEBlurWithLogitsLoss is used for the Discriminator.\n",
    "4.  **Define the optimizers**: Adam is used to optimize the Generator and Discriminator (or Critic) parameters.\n",
    "5.  **Iterate over the dataset**: For each batch of images, the following steps are performed:\n",
    "    *   Generate fake RGB images from the INR images using the Generator.\n",
    "    *   Train the Discriminator (or Critic) to distinguish between real and fake images.\n",
    "    *   Train the Generator to produce more realistic images that can fool the Discriminator (or Critic).\n",
    "6.  **Evaluate the model**: After each epoch, the model is evaluated on a validation set using metrics such as PSNR and SSIM.\n",
    "\n",
    "### 3.2. Training Script Usage\n",
    "\n",
    "To train the model, run the following command:\n",
    "\n",
    "```bash\n",
    "!python train.py --data your_data_directory --epochs 100 --loss mse --batch_size 8\n",
    "```\n",
    "\n",
    "Replace `your_data_directory` with the path to your dataset directory. You can also adjust the other training parameters as needed.\n",
    "\n",
    "For WGAN training, use the `--wgan` flag:\n",
    "\n",
    "```bash\n",
    "!python train.py --data your_data_directory --epochs 100 --loss mse --batch_size 8 --wgan True\n",
    "```\n",
    "\n",
    "### 3.3. TensorBoard Visualization\n",
    "\n",
    "You can use TensorBoard to visualize the training process. To start TensorBoard, run the following command:\n",
    "\n",
    "```bash\n",
    "!tensorboard --logdir runs/\n",
    "```\n",
    "\n",
    "Then, open your browser and navigate to `http://localhost:6006` to view the TensorBoard dashboard."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torchvision import transforms\n",
    "from datasets.data_set import LowLightDataset\n",
    "from models.base_mode import Generator, Discriminator\n",
    "from utils.loss import BCEBlurWithLogitsLoss\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Define the data directory and other parameters\n",
    "data_dir = \"../datasets/LOLdataset\"  # Replace with your dataset path\n",
    "batch_size = 8\n",
    "img_size = (256, 256)\n",
    "num_epochs = 2\n",
    "learning_rate = 0.0002\n",
    "\n",
    "# 3. Define image transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 4. Create the dataset and data loader\n",
    "train_dataset = LowLightDataset(image_dir=data_dir, transform=transform, phase=\"train\")\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "\n",
    "# 5. Instantiate the models\n",
    "generator = Generator().to(device)\n",
    "discriminator = Discriminator().to(device)\n",
    "\n",
    "# 6. Define the loss functions\n",
    "g_loss_fn = BCEBlurWithLogitsLoss().to(device)\n",
    "d_loss_fn = BCEBlurWithLogitsLoss().to(device)\n",
    "\n",
    "# 7. Define the optimizers\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=learning_rate)\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=learning_rate)\n",
    "\n",
    "# 8. Training loop\n",
    "for epoch in range(num_epochs):\n",
    "    for i, (low_images, high_images) in enumerate(train_loader):\n",
    "        low_images = low_images.to(device)\n",
    "        high_images = high_images.to(device)\n",
    "\n",
    "        # Train the discriminator\n",
    "        d_optimizer.zero_grad()\n",
    "        fake_images = generator(low_images)\n",
    "        real_labels = torch.ones(low_images.size(0)).to(device)\n",
    "        fake_labels = torch.zeros(low_images.size(0)).to(device)\n",
    "\n",
    "        d_real_loss = d_loss_fn(discriminator(high_images).squeeze(), real_labels)\n",
    "        d_fake_loss = d_loss_fn(discriminator(fake_images.detach()).squeeze(), fake_labels)\n",
    "        d_loss = (d_real_loss + d_fake_loss) / 2\n",
    "        d_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # Train the generator\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss = g_loss_fn(discriminator(fake_images).squeeze(), real_labels)\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], d_loss: {d_loss.item():.4f}, g_loss: {g_loss.item():.4f}\")\n",
    "\n",
    "# 9. Save the trained models\n",
    "os.makedirs(\"runs/generator\", exist_ok=True)\n",
    "os.makedirs(\"runs/discriminator\", exist_ok=True)\n",
    "torch.save(generator.state_dict(), \"runs/generator/generator.pth\")\n",
    "torch.save(discriminator.state_dict(), \"runs/discriminator/discriminator.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Prediction\n",
    "\n",
    "The `predict.py` script is used to generate RGB images from INR images using the trained Generator model.\n",
    "\n",
    "### 4.1. Prediction Script Usage\n",
    "\n",
    "To perform prediction on a single image or a directory of images, run the following command:\n",
    "\n",
    "```bash\n",
    "!python predict.py --data path_to_image_or_directory --model runs/generator/generator.pth\n",
    "```\n",
    "\n",
    "Replace `path_to_image_or_directory` with the path to the INR image or directory containing INR images. Replace `runs/generator/generator.pth` with the path to the trained Generator model.\n",
    "\n",
    "To use the model with a live camera feed, run the following command:\n",
    "\n",
    "```bash\n",
    "!python predict.py --data 0 --model runs/generator/generator.pth\n",
    "```\n",
    "\n",
    "This will open a window displaying the live camera feed and the generated RGB images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from models.base_mode import Generator\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 1. Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Define the model path and image path\n",
    "model_path = \"runs/generator/generator.pth\"  # Replace with your model path\n",
    "image_path = \"../datasets/LOLdataset/eval15/low/1.png\"  # Replace with your image path\n",
    "\n",
    "# 3. Define image transformations\n",
    "img_size = (256, 256)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 4. Load the model\n",
    "generator = Generator().to(device)\n",
    "generator.load_state_dict(torch.load(model_path))\n",
    "generator.eval()\n",
    "\n",
    "# 5. Load the image\n",
    "img = cv2.imread(image_path)\n",
    "img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "img_tensor = transform(img).unsqueeze(0).to(device)\n",
    "\n",
    "# 6. Perform prediction\n",
    "with torch.no_grad():\n",
    "    generated_img = generator(img_tensor)\n",
    "\n",
    "# 7. Convert the generated image to a numpy array and rescale\n",
    "generated_img_np = generated_img.squeeze().permute(1, 2, 0).cpu().numpy() * 0.5 + 0.5\n",
    "\n",
    "# 8. Display the original and generated images\n",
    "fig, axes = plt.subplots(1, 2, figsize=(10, 5))\n",
    "axes[0].imshow(img)\n",
    "axes[0].set_title(\"Original INR Image\")\n",
    "axes[1].imshow(generated_img_np)\n",
    "axes[1].set_title(\"Generated RGB Image\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Evaluation\n",
    "\n",
    "The performance of the trained model can be evaluated using metrics such as Peak Signal-to-Noise Ratio (PSNR) and Structural Similarity Index Measure (SSIM).\n",
    "\n",
    "### 5.1. PSNR\n",
    "\n",
    "PSNR measures the quality of the generated image compared to the ground truth RGB image. Higher PSNR values indicate better image quality.\n",
    "\n",
    "### 5.2. SSIM\n",
    "\n",
    "SSIM measures the structural similarity between the generated image and the ground truth RGB image. SSIM values range from -1 to 1, with higher values indicating better similarity."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from datasets.data_set import LowLightDataset\n",
    "from models.base_mode import Generator\n",
    "from torcheval.metrics.functional import peak_signal_noise_ratio\n",
    "from utils.misic import ssim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. Set the device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# 2. Define the model path and data directory\n",
    "model_path = \"runs/generator/generator.pth\"  # Replace with your model path\n",
    "data_dir = \"../datasets/LOLdataset\"  # Replace with your dataset path\n",
    "\n",
    "# 3. Define image transformations\n",
    "img_size = (256, 256)\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize(img_size),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))\n",
    "])\n",
    "\n",
    "# 4. Create the test dataset and data loader\n",
    "test_dataset = LowLightDataset(image_dir=data_dir, transform=transform, phase=\"test\")\n",
    "test_loader = DataLoader(test_dataset, batch_size=1, shuffle=False)\n",
    "\n",
    "# 5. Load the model\n",
    "generator = Generator().to(device)\n",
    "generator.load_state_dict(torch.load(model_path))\n",
    "generator.eval()\n",
    "\n",
    "# 6. Evaluate the model\n",
    "psnr_values = []\n",
    "ssim_values = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for low_images, high_images in test_loader:\n",
    "        low_images = low_images.to(device)\n",
    "        high_images = high_images.to(device)\n",
    "\n",
    "        fake_images = generator(low_images)\n",
    "\n",
    "        psnr = peak_signal_noise_ratio(fake_images, high_images).item()\n",
    "        ssim_val = ssim(fake_images, high_images).item()\n",
    "\n",
    "        psnr_values.append(psnr)\n",
    "        ssim_values.append(ssim_val)\n",
    "\n",
    "# 7. Print the results\n",
    "print(f\"PSNR: {sum(psnr_values) / len(psnr_values):.4f}\")\n",
    "print(f\"SSIM: {sum(ssim_values) / len(ssim_values):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Conclusion\n",
    "\n",
    "This notebook provides a step-by-step guide to training and using a neural network for converting INR images to RGB images. By following the instructions in this notebook, you can train your own INR2RGB model and use it to enhance night vision capabilities for various applications.\n",
    "\n",
    "### 6.1. Future Directions\n",
    "\n",
    "*   Experiment with different model architectures and loss functions.\n",
    "*   Train the model on larger and more diverse datasets.\n",
    "*   Explore the use of transfer learning to improve the performance of the model.\n",
    "*   Implement real-time INR2RGB conversion for live video feeds."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
